{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T14:11:12.189700Z",
     "start_time": "2018-02-13T14:11:12.170551Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T14:13:49.722255Z",
     "start_time": "2018-02-13T14:13:49.708661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39485789673108734"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "PRICE_STD = 0.7459273872548303\n",
    "def compute_loss(pred, y):\n",
    "    return np.sqrt(np.mean(np.square(pred - y)))\n",
    "\n",
    "y_pred=0.52*(con*0.5+ft1*0.5)+0.48*fm\n",
    "compute_loss(y.reshape(-1)*PRICE_STD, y_pred*PRICE_STD)\n",
    "# print(con.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T23:06:14.898070Z",
     "start_time": "2018-02-13T23:06:13.870850Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a=pd.read_csv('submission_gru_lstm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T08:19:17.477761Z",
     "start_time": "2018-02-14T08:19:17.448624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4250"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "400+1200+900+1750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T03:33:29.149350Z",
     "start_time": "2018-02-12T03:33:29.141032Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a=0.642724820378248\n",
    "b=0.6530566788062586\n",
    "c=0.819184552611957\n",
    "d=a+b+c\n",
    "print(a/d*3)\n",
    "print(b/d*3)\n",
    "print(c/d*3)\n",
    "print(a/d*3+b/d*3+c/d*3)\n",
    "# [2018-02-12 10:17:29,932] INFO:       weight1: 0.3448079429985605\n",
    "# [2018-02-12 10:17:29,932] INFO:       weight2: 0.7375577997303264\n",
    "# [2018-02-12 10:17:29,932] INFO:       weight3: 0.6047638788389827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T03:04:40.268928Z",
     "start_time": "2018-02-11T03:04:40.235105Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         wb = wordbatch.WordBatch(normalize_text=None,extractor=(WordBag, {\n",
    "#             \"hash_ngrams\": 2,\n",
    "#             \"hash_ngrams_weights\": [name_w1, name_w2],\n",
    "#             \"hash_size\": 2 ** 28,\n",
    "#             \"norm\": None,\n",
    "#             \"tf\": 'binary',\n",
    "#             \"idf\": None,\n",
    "#         }), procs=8)\n",
    "#         wb.dictionary_freeze = True\n",
    "#         X_name = wb.fit_transform(merge['name'])\n",
    "#         del (wb)\n",
    "#         X_name = X_name[:, np.array(np.clip(X_name.getnnz(axis=0) - 2, 0, 1), dtype=bool)]\n",
    "#         print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T02:26:33.462975Z",
     "start_time": "2018-02-11T02:26:33.428851Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfAll.loc[0,'category_name']='*)(?!#$#@(@#??@#$??#@??$?#?!@#$?!/'*1000\n",
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T02:13:57.678932Z",
     "start_time": "2018-02-11T02:13:57.651738Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfAll['item_condition_id'].fillna(value=1, inplace=True)\n",
    "dfAll['item_condition_id']=dfAll['item_condition_id'].astype('int32')\n",
    "dfAll['shipping'].fillna(value=0, inplace=True)\n",
    "dfAll['shipping']=dfAll['shipping'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T02:19:54.419679Z",
     "start_time": "2018-02-11T02:19:54.399934Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfAll.head()\n",
    "# dfAll['item_condition_id']\n",
    "# dfAll['shipping']\n",
    "# dfAll['item_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @Time    : 1/25/18 5:03 PM\n",
    "# @Author  : LeeYun\n",
    "# @File    : model_ensemble.py\n",
    "'''Description :\n",
    "0.40146\n",
    "'''\n",
    "import os, string, pickle, re, time, gc, multiprocessing, wordbatch, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from fastcache import clru_cache as lru_cache\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras import backend, optimizers, callbacks \n",
    "from keras.models import Model\n",
    "from keras.backend import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Lambda\n",
    "from wordbatch.models import FM_FTRL\n",
    "from wordbatch.extractors import WordBag\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "DEVELOP = False\n",
    "EPOCH = 2\n",
    "\n",
    "if DEVELOP:\n",
    "    TRAIN_SIZE = 10000\n",
    "else:\n",
    "    TRAIN_SIZE = 1481661\n",
    "PRICE_MEAN = 2.98081628517\n",
    "PRICE_STD = 0.7459273872548303\n",
    "NumWords = 50000\n",
    "THREAD = 4\n",
    "MISSVALUE = 'missvalue'\n",
    "\n",
    "# 0.41717 ensemble\n",
    "# 0.42192 single\n",
    "param_space_best_vanila_con1d = {\n",
    "    'denselayer_units': 244,\n",
    "    'description_Len': 85,\n",
    "    'name_Len': 10,\n",
    "    'embed_name': 58,\n",
    "    'embed_desc': 52,\n",
    "    'embed_brand': 22,\n",
    "    'embed_cat_2': 14,\n",
    "    'embed_cat_3': 37,\n",
    "    'name_filter': 114,\n",
    "    'desc_filter': 96,\n",
    "    'name_filter_size': 4,\n",
    "    'desc_filter_size': 4,\n",
    "    'lr': 0.003924230325700921,\n",
    "    'batch_size': 933,\n",
    "    'dense_drop': 0.009082372998548981,\n",
    "}\n",
    "\n",
    "# 0.41855 ensemble\n",
    "# 0.42368 single\n",
    "param_space_best_vanila_GRU = {\n",
    "    'denselayer_units': 232,\n",
    "    'description_Len': 85,\n",
    "    'name_Len': 10,\n",
    "    'embed_name': 49,\n",
    "    'embed_desc': 56,\n",
    "    'embed_brand': 29,\n",
    "    'embed_cat_2': 12,\n",
    "    'embed_cat_3': 37,\n",
    "    'rnn_dim_name': 23,\n",
    "    'rnn_dim_desc': 24,\n",
    "    'lr': 0.0036379197189225143,\n",
    "    'batch_size': 597,\n",
    "    'dense_drop': 0.00498597880773485,\n",
    "}\n",
    "\n",
    "# 0.41217\n",
    "param_space_best_FM_FTRL = {\n",
    "    'alpha': 0.027337603769727097,\n",
    "    'beta': 0.000696315247353981,\n",
    "    'L1': 3.261127986664043e-05,\n",
    "    'L2': 0.01154650635257946,\n",
    "    'alpha_fm': 0.01551798270107723,\n",
    "    'init_fm': 0.02798274281370472,\n",
    "    'D_fm': 248,\n",
    "    'e_noise': 0.0007545501112447097,\n",
    "    'iters': 10,\n",
    "}\n",
    "\n",
    "param_space_best_WordBatch = {\n",
    "    'desc_w1': 1.3740067995315037,\n",
    "    'desc_w2': 1.0248685266832964,\n",
    "    'desc_w3': 0.7,\n",
    "    'name_w1': 2.1385527373939834,\n",
    "    'name_w2': 0.3894761681383836,\n",
    "}\n",
    "\n",
    "param_space_best_ensemble = {\n",
    "    'GRU_weight': 0.5,\n",
    "    'conv1d_weight': 0.5,\n",
    "    'FM_FTRL_weight': 1.1,\n",
    "}\n",
    "ensemble_total_weights = param_space_best_ensemble['GRU_weight'] + param_space_best_ensemble['conv1d_weight'] + \\\n",
    "                         param_space_best_ensemble['FM_FTRL_weight']\n",
    "param_space_best_ensemble['GRU_weight'] /= ensemble_total_weights\n",
    "param_space_best_ensemble['conv1d_weight'] /= ensemble_total_weights\n",
    "param_space_best_ensemble['FM_FTRL_weight'] /= ensemble_total_weights\n",
    "print(param_space_best_ensemble)\n",
    "\n",
    "# # regularize name\n",
    "# name_list = (\n",
    "#     (['lebron soldier'], 'lebron_soldier'),\n",
    "#     (['air force', 'air forces'], 'air_force'),\n",
    "#     (['air max'], 'air_max'),\n",
    "#     (['vs pink'], 'vs_pink'),\n",
    "#     (['victoria secret', 'victoria secrets', 'v . s .'], 'victoria_\\'_s_secret'),\n",
    "#     (['f 21'], 'forever_21'),\n",
    "#     (['lulu lemon', 'lulu'], 'lululemon'),\n",
    "#     (['a . e .'], 'american_eagle'),\n",
    "# )\n",
    "# # map from keyword to brand\n",
    "# brand_list = (\n",
    "#     (['lebron_soldier', 'kyrie', 'air_force', 'air_max'], 'nike'),\n",
    "#     (['vs_pink'], 'pink'),\n",
    "#     (['vsx'], 'victoria \\' s secret'),\n",
    "#     (['llr'], 'lularoe'),\n",
    "#     (['ipad', 'ipod', 'macbook', 'iphone'], 'apple'),\n",
    "#     (['wii', 'gameboy', 'zelda', 'gamecube', ], 'nintendo'),\n",
    "#     (['ae', 'aeo', 'aerie'], 'american eagle'),\n",
    "#     (['disneyland', 'minnie', 'mickey', 'woody', 'tsum'], 'disney'),\n",
    "# )\n",
    "# name_dict, brand_dict = {}, {}\n",
    "# for keywords, value in name_list:\n",
    "#     for item in keywords:\n",
    "#         name_dict[item] = value\n",
    "# for keywords, brand in brand_list:\n",
    "#     for item in keywords:\n",
    "#         brand_dict[item] = brand + '2'\n",
    "# del name_list, brand_list\n",
    "#\n",
    "#\n",
    "# # Create a regular expression  from the dictionary keys\n",
    "# @lru_cache(1024)\n",
    "# def brand_check(brand: str, name: str):\n",
    "#     if brand != MISSVALUE: return brand\n",
    "#     # check brand\n",
    "#     if name != MISSVALUE:\n",
    "#         # regularize name\n",
    "#         name = regex.sub(lambda mo: name_dict[mo.string[mo.start():mo.end()]], name)\n",
    "#         words = name.split(\" \")\n",
    "#         for word in words:\n",
    "#             if word in brand_dict:\n",
    "#                 return brand_dict[word]\n",
    "#     return brand\n",
    "#\n",
    "#\n",
    "# def brand_fill(df: pd.DataFrame):\n",
    "#     # fill brand\n",
    "#     global regex\n",
    "#     regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, name_dict.keys())))\n",
    "#     # _save('Data/brand_dict_name_dict_regex', [brand_dict, name_dict, regex])\n",
    "#     return df.apply(lambda x: brand_check(x.values[0], x.values[1]), axis=1)\n",
    "\n",
    "\n",
    "@lru_cache(1024)\n",
    "def split_cat(text: str):\n",
    "    text = text.split(\"/\")\n",
    "    if len(text) >= 2:\n",
    "        return text[0], text[1]\n",
    "    else:\n",
    "        return text[0], MISSVALUE\n",
    "\n",
    "\n",
    "@lru_cache(1024)\n",
    "def len_splt(str: str):\n",
    "    return len(str.split(' '))\n",
    "\n",
    "\n",
    "def prepare_batches(seq: np.ndarray, step: int):\n",
    "    n = len(seq)\n",
    "    res = []\n",
    "    for i in range(0, n, step):\n",
    "        res.append(seq[i:i + step])\n",
    "    return res\n",
    "\n",
    "\n",
    "def text_processor1(text: pd.Series):\n",
    "    return text.str.lower(). \\\n",
    "        str.replace(r'([a-z]+|[0-9]+)', r' \\1 '). \\\n",
    "        str.replace(r'([0-9])( *)\\.( *)([0-9])', r'\\1.\\4'). \\\n",
    "        str.replace('\\s+', ' '). \\\n",
    "        str.strip()\n",
    "\n",
    "\n",
    "def text_processor2(text: pd.Series):\n",
    "    return text.str.lower(). \\\n",
    "        str.replace(r'[^\\.!?@#&%$/\\\\0-9a-z]', ' '). \\\n",
    "        str.replace(r'(\\.|!|\\?|@|#|&|%|\\$|/|\\\\|[0-9]+)', r' \\1 '). \\\n",
    "        str.replace(r'([0-9])( *)\\.( *)([0-9])', r'\\1.\\4'). \\\n",
    "        str.replace('\\s+', ' '). \\\n",
    "        str.strip()\n",
    "\n",
    "\n",
    "def intersect_cnt(dfs: np.ndarray):\n",
    "    intersect = np.empty(dfs.shape[0])\n",
    "    for i in range(dfs.shape[0]):\n",
    "        obs_tokens = set(dfs[i, 0].split(\" \"))\n",
    "        target_tokens = set(dfs[i, 1].split(\" \"))\n",
    "        intersect[i] = len(obs_tokens.intersection(target_tokens)) / (obs_tokens.__len__() + 1)\n",
    "    return intersect\n",
    "\n",
    "import random\n",
    "def random_words(wc):\n",
    "    wc=int(wc)\n",
    "    ret=[]\n",
    "    for _ in range(wc):\n",
    "        ret.append(''.join(random.sample(string.ascii_letters, random.randrange(3, 15))))\n",
    "    return ret\n",
    "\n",
    "def shuffle_strings(x):\n",
    "    global salts\n",
    "    x = repr(x).split(' ')\n",
    "    i=np.random.randint(len(x))\n",
    "    j=np.random.randint(100000)\n",
    "    x[i]=salts[j]\n",
    "    return ' '.join(x)\n",
    "\n",
    "def shuffle_series(data):\n",
    "    global salts\n",
    "    for name in ['name','item_description']:\n",
    "        data[name]=data[name].apply(shuffle_strings)\n",
    "    data.loc[0,'category_name']='a'\n",
    "    data.loc[1,'category_name']='a/b'\n",
    "    data.loc[2,'category_name']='a/b/c'\n",
    "    data.loc[3,'category_name']='asd/asediafi/as'\n",
    "    data.loc[4,'category_name']='scas/asdc/asdc'\n",
    "    data.loc[5,'category_name']='Men/shirt'\n",
    "    data.loc[6,'category_name']='Men'\n",
    "    data.loc[7,'category_name']='Men/shirt/shoe'\n",
    "    data.loc[8,'category_name']='a'\n",
    "    data.loc[10,'brand_name']='a'\n",
    "    data.loc[11,'brand_name']='a b'\n",
    "    data.loc[12,'brand_name']='a/b/c'\n",
    "    data.loc[13,'brand_name']='asdasediafi 88as'\n",
    "    data.loc[14,'brand_name']='scas*asdc & asdc'\n",
    "    data.loc[15,'brand_name']='Men - shirt'\n",
    "    data.loc[16,'brand_name']='Men'\n",
    "    data.loc[17,'brand_name']='Men3shirtshoe'\n",
    "    data.loc[18,'brand_name']='a'\n",
    "    return data\n",
    "\n",
    "def get_extract_feature():\n",
    "    def read_file(name: str):\n",
    "        source = '../input/%s.tsv' % name\n",
    "        df = pd.read_table(source, engine='c')\n",
    "        return df\n",
    "\n",
    "    def textclean(merge: pd.DataFrame):\n",
    "        columns = ['name', 'category_name', 'brand_name', 'item_description']\n",
    "        for col in columns: merge[col].fillna(value=MISSVALUE, inplace=True)\n",
    "        merge['item_condition_id'].fillna(value=1, inplace=True)\n",
    "        merge['shipping'].fillna(value=0, inplace=True)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        columns = ['item_description', 'name']\n",
    "        p = multiprocessing.Pool(THREAD)\n",
    "        length = merge.shape[0]\n",
    "        len1, len2, len3 = length // 4, length // 2, (length // 4) * 3\n",
    "        for col in columns:\n",
    "            print(col)\n",
    "            slices = [merge[col][:len1], merge[col][len1:len2], merge[col][len2:len3], merge[col][len3:]]\n",
    "            dfvalue = []\n",
    "            dfs = p.imap(text_processor1, slices)\n",
    "            for df in dfs: dfvalue.append(df.values)\n",
    "            merge[col] = np.concatenate((dfvalue[0], dfvalue[1], dfvalue[2], dfvalue[3]))\n",
    "        p.close();\n",
    "        slices, dfvalue, dfs, df, p = None, None, None, None, None;\n",
    "        gc.collect()\n",
    "\n",
    "        print('[{}] clean item_description completed'.format(time.time() - start_time))\n",
    "        \n",
    "        columns = ['brand_name', 'category_name']\n",
    "        p = multiprocessing.Pool(THREAD)\n",
    "        length = merge.shape[0]\n",
    "        len1, len2, len3 = length // 4, length // 2, (length // 4) * 3\n",
    "        for col in columns:\n",
    "            slices = [merge[col][:len1], merge[col][len1:len2], merge[col][len2:len3], merge[col][len3:]]\n",
    "            dfvalue = []\n",
    "            dfs = p.imap(text_processor2, slices)\n",
    "            for df in dfs: dfvalue.append(df.values)\n",
    "            merge[col] = np.concatenate((dfvalue[0], dfvalue[1], dfvalue[2], dfvalue[3]))\n",
    "        p.close();\n",
    "        slices, dfvalue, dfs, df, p = None, None, None, None, None;\n",
    "        gc.collect()\n",
    "\n",
    "        merge['category_1'], merge['category_2'] = zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n",
    "        return merge\n",
    "\n",
    "    def get_cleaned_data():\n",
    "        dftrain = read_file('train')\n",
    "        dftest = read_file('test')\n",
    "\n",
    "        # global salts\n",
    "        # # Make a stage 2 test by copying test five times...\n",
    "        # test1 = dftest.copy()\n",
    "        # test2 = dftest.copy()\n",
    "        # test3 = dftest.copy()\n",
    "        # test4 = dftest.copy()\n",
    "        # test5 = dftest.copy()\n",
    "        # dftest = pd.concat([test1, test2, test3, test4, test5], axis=0)\n",
    "        # test1 = None\n",
    "        # test2 = None\n",
    "        # test3 = None\n",
    "        # test4 = None\n",
    "        # test5 = None\n",
    "        #\n",
    "        # word_counts = 100000  # Word Numbers in Stage I denpends on the tokenizer\n",
    "        # salts = random_words(word_counts)\n",
    "        # dftest=shuffle_series(dftest)\n",
    "        # print('shuffle_series done')\n",
    "        # del salts\n",
    "\n",
    "        dftrain = dftrain[dftrain.price != 0]\n",
    "        dfAll = pd.concat((dftrain, dftest), ignore_index=True)\n",
    "        if DEVELOP: dfAll = dfAll[:20000]\n",
    "        dfAll = textclean(dfAll)\n",
    "        submission: pd.DataFrame = dftest[['test_id']]\n",
    "        dftrain, dftest = None, None;\n",
    "        gc.collect()\n",
    "        return dfAll, submission\n",
    "\n",
    "    def add_Frec_feat(dfAll: pd.DataFrame, col: str):\n",
    "        s = dfAll[col].value_counts()\n",
    "        s[MISSVALUE] = 0\n",
    "        dfAll = dfAll.merge(s.to_frame(name=col + '_Frec'), left_on=col, right_index=True, how='left')\n",
    "        s = None\n",
    "        return dfAll\n",
    "\n",
    "    dfAll, submission = get_cleaned_data()\n",
    "    print('data cleaned')\n",
    "\n",
    "    # add the Frec features\n",
    "    columns = ['brand_name']\n",
    "    print('add the Frec features')\n",
    "    for col in columns: dfAll = add_Frec_feat(dfAll, col)\n",
    "\n",
    "    # intersection count between 'item_description','name','brand_name','category_name'\n",
    "    columns = [['brand_name', 'name'], ['brand_name', 'item_description']]\n",
    "    p = multiprocessing.Pool(THREAD)\n",
    "    length = dfAll.shape[0]\n",
    "    len1, len2, len3 = length // 4, length // 2, (length // 4) * 3\n",
    "    for col in columns:\n",
    "        slices = [dfAll[col].values[:len1], dfAll[col].values[len1:len2], dfAll[col].values[len2:len3],\n",
    "                  dfAll[col].values[len3:]]\n",
    "        dfvalue = []\n",
    "        dfs = p.imap(intersect_cnt, slices)\n",
    "        for df in dfs: dfvalue.append(df)\n",
    "        dfAll['%s_%s_Intsct' % (col[0], col[1])] = np.concatenate((dfvalue[0], dfvalue[1], dfvalue[2], dfvalue[3]))\n",
    "    p.close();\n",
    "    slices, dfvalue, dfs, df, p = None, None, None, None, None;\n",
    "    gc.collect()\n",
    "\n",
    "    # remove brand_name that only appeared in dfTest\n",
    "    dftrain = dfAll[:TRAIN_SIZE]\n",
    "    columns = ['category_name', 'brand_name']\n",
    "    for col in columns:\n",
    "        mask = ~dfAll[col].isin(dftrain[col].unique())\n",
    "        dfAll.loc[mask, col] = MISSVALUE\n",
    "        print('%s missvalue %d' % (col, mask.sum()))\n",
    "\n",
    "    # count item_description length\n",
    "    dfAll['item_description_wordLen'] = dfAll.item_description.apply(lambda x: len_splt(x))\n",
    "    # nomalize price\n",
    "    y_train = ((np.log1p(dfAll.price) - PRICE_MEAN) / PRICE_STD).values[:TRAIN_SIZE].reshape(-1, 1).astype(np.float32)\n",
    "    dfAll.drop(['price', 'test_id', 'train_id'], axis=1, inplace=True)\n",
    "    return dfAll, submission, y_train\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "merge, submission, y_train = get_extract_feature()\n",
    "print('[{}] data preparation done.'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-10T12:35:41.042803Z",
     "start_time": "2018-02-10T12:35:10.897490Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "def Label_Encoder(df: pd.Series):\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(df)\n",
    "\n",
    "\n",
    "def Item_Tokenizer(df: pd.Series):\n",
    "    # do not concern the words only appeared in dftest\n",
    "    tok_raw = Tokenizer(num_words=NumWords, filters='')\n",
    "    tok_raw.fit_on_texts(df[:TRAIN_SIZE])\n",
    "    return tok_raw.texts_to_sequences(df), min(tok_raw.word_counts.__len__() + 1, NumWords)\n",
    "\n",
    "\n",
    "def Preprocess_features(merge: pd.DataFrame, start_time):\n",
    "    merge['item_condition_id'] = merge['item_condition_id'].astype('category')\n",
    "    print('[{}] Convert categorical completed'.format(time.time() - start_time))\n",
    "\n",
    "    Item_size = {}\n",
    "    # Label_Encoder brand_name + category\n",
    "    columns = ['category_1', 'category_2', 'category_name', 'brand_name']\n",
    "    p = multiprocessing.Pool(THREAD)\n",
    "    dfs = p.imap(Label_Encoder, [merge[col] for col in columns])\n",
    "    for col, df in zip(columns, dfs):\n",
    "        merge['Lb_' + col] = df\n",
    "        Item_size[col] = merge['Lb_' + col].max() + 1\n",
    "    print('[{}] Label Encode `brand_name` and `categories` completed.'.format(time.time() - start_time))\n",
    "    p.close();\n",
    "    dfs, df, p = None, None, None;\n",
    "    gc.collect()\n",
    "\n",
    "    # sequance item_description,name\n",
    "    columns = ['item_description', 'name']\n",
    "    p = multiprocessing.Pool(THREAD)\n",
    "    dfs = p.imap(Item_Tokenizer, [merge[col] for col in columns])\n",
    "    for col, df in zip(columns, dfs):\n",
    "        merge['Seq_' + col], Item_size[col] = df\n",
    "    print('[{}] sequance `item_description` and `name` completed.'.format(time.time() - start_time))\n",
    "    print(Item_size)\n",
    "    p.close();\n",
    "    dfs, df, p = None, None, None;\n",
    "    gc.collect()\n",
    "\n",
    "    # hand feature\n",
    "    columns = ['brand_name_Frec', 'item_description_wordLen']\n",
    "    for col in columns:\n",
    "        merge[col] = np.log1p(merge[col])\n",
    "        merge[col] = merge[col] / merge[col].max()\n",
    "\n",
    "    # reduce memory\n",
    "    for col in merge.columns:\n",
    "        if str(merge[col].dtype) == 'int64':\n",
    "            merge[col] = merge[col].astype('int32')\n",
    "        elif str(merge[col].dtype) == 'float64':\n",
    "            merge[col] = merge[col].astype('float32')\n",
    "\n",
    "    hand_feature = ['brand_name_Frec',\n",
    "                    'item_description_wordLen',\n",
    "                    'brand_name_name_Intsct',\n",
    "                    'brand_name_item_description_Intsct']\n",
    "    return merge, Item_size, hand_feature\n",
    "\n",
    "def embed(inputs, size, dim):\n",
    "    std = np.sqrt(2 / dim)\n",
    "    emb = tf.Variable(tf.random_uniform([size, dim], -std, std))\n",
    "    lookup = tf.nn.embedding_lookup(emb, inputs)\n",
    "    return lookup\n",
    "\n",
    "def conv1d(inputs, num_filters, filter_size, padding='same', strides=1):\n",
    "    he_std = np.sqrt(2 / (filter_size * num_filters))\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=inputs, filters=num_filters, padding=padding,\n",
    "        kernel_size=filter_size,\n",
    "        strides=strides,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=he_std))\n",
    "    return out\n",
    "\n",
    "def dense(X, size, activation=None):\n",
    "    he_std = np.sqrt(2 / int(X.shape[1]))\n",
    "    out = tf.layers.dense(X, units=size, activation=activation,\n",
    "                          kernel_initializer=tf.random_normal_initializer(stddev=he_std))\n",
    "    return out\n",
    "\n",
    "\n",
    "def CountVector(df):\n",
    "    wb = CountVectorizer()\n",
    "    return wb.fit_transform(df).astype(np.int32)\n",
    "\n",
    "def Get_CountVectorizer(merge: pd.DataFrame):\n",
    "    columns = ['category_2', 'category_name', 'brand_name']\n",
    "    p = multiprocessing.Pool(THREAD)\n",
    "    dfs = p.imap(CountVector, [merge[col] for col in columns])\n",
    "    results = []\n",
    "    for col, df in zip(columns, dfs): results.append(df)\n",
    "    p.close();\n",
    "    dfs, df, p = None, None, None;\n",
    "    gc.collect()\n",
    "    return results[0], results[1], results[2]\n",
    "    \n",
    "def LabelBinarize(df):\n",
    "    lb = LabelBinarizer(sparse_output=True)\n",
    "    return lb.fit_transform(df).astype(np.int32)\n",
    "\n",
    "def Get_LabelBinarizer(merge: pd.DataFrame):\n",
    "    columns = ['category_1', 'category_name', 'brand_name']\n",
    "    p = multiprocessing.Pool(THREAD)\n",
    "    dfs = p.imap(LabelBinarize, [merge[col] for col in columns])\n",
    "    results = []\n",
    "    for col, df in zip(columns, dfs): results.append(df)\n",
    "    p.close();\n",
    "    dfs, df, p = None, None, None;\n",
    "    gc.collect()\n",
    "    return results[0], results[1], results[2]\n",
    "\n",
    "\n",
    "def Split_Train_Test_NN(data: pd.DataFrame, hand_feature):\n",
    "    param_dict = param_space_best_vanila_con1d\n",
    "    X_seq_item_description = pad_sequences(data['Seq_item_description'], maxlen=param_dict['description_Len'])\n",
    "    X_seq_name = pad_sequences(data['Seq_name'], maxlen=param_dict['name_Len'])\n",
    "\n",
    "    X_brand_name = data.Lb_brand_name.values.reshape(-1, 1)\n",
    "    X_category_1 = data.Lb_category_1.values.reshape(-1, 1)\n",
    "    X_category_2 = data.Lb_category_2.values.reshape(-1, 1)\n",
    "    X_category_name = data.Lb_category_name.values.reshape(-1, 1)\n",
    "    X_item_condition_id = (data.item_condition_id.values.astype(np.int32) - 1).reshape(-1, 1)\n",
    "    X_shipping = ((data.shipping.values - data.shipping.values.mean()) / data.shipping.values.std()).reshape(-1, 1)\n",
    "    X_hand_feature = (data[hand_feature].values - data[hand_feature].values.mean(axis=0)) / data[\n",
    "        hand_feature].values.std(axis=0)\n",
    "\n",
    "    X_train = dict(\n",
    "        X_seq_item_description=X_seq_item_description[:TRAIN_SIZE],\n",
    "        X_seq_name=X_seq_name[:TRAIN_SIZE],\n",
    "        X_brand_name=X_brand_name[:TRAIN_SIZE],\n",
    "        X_category_1=X_category_1[:TRAIN_SIZE],\n",
    "        X_category_2=X_category_2[:TRAIN_SIZE],\n",
    "        X_category_name=X_category_name[:TRAIN_SIZE],\n",
    "        X_item_condition_id=X_item_condition_id[:TRAIN_SIZE],\n",
    "        X_shipping=X_shipping[:TRAIN_SIZE],\n",
    "        X_hand_feature=X_hand_feature[:TRAIN_SIZE],\n",
    "    )\n",
    "    X_valid = dict(\n",
    "        X_seq_item_description=X_seq_item_description[TRAIN_SIZE:],\n",
    "        X_seq_name=X_seq_name[TRAIN_SIZE:],\n",
    "        X_brand_name=X_brand_name[TRAIN_SIZE:],\n",
    "        X_category_1=X_category_1[TRAIN_SIZE:],\n",
    "        X_category_2=X_category_2[TRAIN_SIZE:],\n",
    "        X_category_name=X_category_name[TRAIN_SIZE:],\n",
    "        X_item_condition_id=X_item_condition_id[TRAIN_SIZE:],\n",
    "        X_shipping=X_shipping[TRAIN_SIZE:],\n",
    "        X_hand_feature=X_hand_feature[TRAIN_SIZE:],\n",
    "    )\n",
    "    X_seq_item_description, X_seq_name, X_brand_name, X_category_1, X_category_2, X_category_name, \\\n",
    "    X_item_condition_id, X_shipping, X_hand_feature = None, None, None, None, None, None, None, None, None\n",
    "    return X_train, X_valid\n",
    "\n",
    "\n",
    "# def inductive_brand(merge: pd.DataFrame):\n",
    "#     # inductive the missing brand from name\n",
    "#     print((merge['brand_name'] == MISSVALUE).sum())\n",
    "#\n",
    "#     # supplement name_dict and brand_dict\n",
    "#     brand_frec = merge['brand_name'].value_counts()[1:200]  # 150, 200, 250\n",
    "#     for i in range(brand_frec.shape[0]):\n",
    "#         text = brand_frec.index.values[i]\n",
    "#         text2 = '_'.join(text.split(' '))\n",
    "#         if text != text2: name_dict[text] = text2\n",
    "#         brand_dict[text2] = text + '2'\n",
    "#\n",
    "#     common_word = ['pink', 'express', 'guess', 'beats', 'supreme']\n",
    "#     for item in common_word:\n",
    "#         if brand_dict[item]: brand_dict.pop(item)\n",
    "#\n",
    "#     p = multiprocessing.Pool(THREAD)\n",
    "#     length = merge.shape[0]\n",
    "#     len1, len2, len3 = length // 4, length // 2, (length // 4) * 3\n",
    "#     slices = [merge[:len1], merge[len1:len2], merge[len2:len3], merge[len3:]]\n",
    "#     dfvalue = []\n",
    "#     dfs = p.imap(brand_fill, slices)\n",
    "#     for df in dfs: dfvalue.append(df)\n",
    "#     brand_name = pd.concat((dfvalue[0], dfvalue[1], dfvalue[2], dfvalue[3]), ignore_index=True)\n",
    "#     print((brand_name == MISSVALUE).sum())\n",
    "#     p.close();\n",
    "#     slices, dfvalue, dfs, df, p = None, None, None, None, None;\n",
    "#     gc.collect()\n",
    "#     return brand_name\n",
    "\n",
    "\n",
    "def Split_Train_Test_FTRL(merge: pd.DataFrame, hand_feature, start_time):\n",
    "    desc_w1 = param_space_best_WordBatch['desc_w1']\n",
    "    desc_w2 = param_space_best_WordBatch['desc_w2']\n",
    "    desc_w3 = param_space_best_WordBatch['desc_w3']\n",
    "    name_w1 = param_space_best_WordBatch['name_w1']\n",
    "    name_w2 = param_space_best_WordBatch['name_w2']\n",
    "\n",
    "    # merge['brand_name'] = inductive_brand(merge[['brand_name', 'name']])\n",
    "\n",
    "    wb = wordbatch.WordBatch(normalize_text=None, extractor=(WordBag, {\n",
    "        \"hash_ngrams\": 2,\n",
    "        \"hash_ngrams_weights\": [name_w1, name_w2],\n",
    "        \"hash_size\": 2 ** 28,\n",
    "        \"norm\": None,\n",
    "        \"tf\": 'binary',\n",
    "        \"idf\": None,\n",
    "    }), procs=8)\n",
    "    wb.dictionary_freeze = True\n",
    "    X_name = wb.fit_transform(merge['name']).astype(np.float32)\n",
    "    del wb\n",
    "    merge.drop(['name'], axis=1, inplace=True)\n",
    "    X_name = X_name[:, np.array(np.clip(X_name[:TRAIN_SIZE].getnnz(axis=0) - 2, 0, 1), dtype=bool)]\n",
    "    print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))\n",
    "\n",
    "    wb = wordbatch.WordBatch(normalize_text=None, extractor=(WordBag, {\n",
    "        \"hash_ngrams\": 3,\n",
    "        \"hash_ngrams_weights\": [desc_w1, desc_w2, desc_w3],\n",
    "        \"hash_size\": 2 ** 28,\n",
    "        \"norm\": \"l2\",\n",
    "        \"tf\": 1.0,\n",
    "        \"idf\": None\n",
    "    }), procs=8)\n",
    "    wb.dictionary_freeze = True\n",
    "    X_description_train = wb.fit_transform(merge['item_description'][:TRAIN_SIZE]).astype(np.float32)\n",
    "    mask=np.array(np.clip(X_description_train.getnnz(axis=0) - 5, 0, 1), dtype=bool)\n",
    "    X_description_train = X_description_train[:, mask]\n",
    "    print('X_description_train done')\n",
    "    valid_len=merge.shape[0]-TRAIN_SIZE\n",
    "    valid_len1,valid_len2=int(valid_len/3),int(valid_len*2/3)\n",
    "    X_description_test1 = wb.fit_transform(merge['item_description'][TRAIN_SIZE:TRAIN_SIZE+valid_len1]).astype(np.float32)\n",
    "    X_description_test1 = X_description_test1[:, mask]\n",
    "    print('X_description_test1 done')\n",
    "    X_description_test2 = wb.fit_transform(merge['item_description'][TRAIN_SIZE+valid_len1:TRAIN_SIZE+valid_len2]).astype(np.float32)\n",
    "    X_description_test2 = X_description_test2[:, mask]\n",
    "    print('X_description_test2 done')\n",
    "    X_description_test3 = wb.fit_transform(merge['item_description'][TRAIN_SIZE+valid_len2:]).astype(np.float32)\n",
    "    X_description_test3 = X_description_test3[:, mask]\n",
    "    print('X_description_test3 done')\n",
    "    del wb,mask\n",
    "    merge.drop(['item_description'], axis=1, inplace=True)\n",
    "    print(X_description_train.shape, X_description_test1.shape, X_description_test2.shape, X_description_test3.shape)\n",
    "    print('[{}] Vectorize `item_description` completed.'.format(time.time() - start_time))\n",
    "\n",
    "    X_category2, X_category3, X_brand2 = Get_CountVectorizer(merge)\n",
    "    X_category1, X_category4, X_brand = Get_LabelBinarizer(merge)\n",
    "    merge.drop(['category_1', 'category_2', 'category_name', 'brand_name'], axis=1, inplace=True)\n",
    "    print('[{}] Count vectorize `categories` completed.'.format(time.time() - start_time))\n",
    "\n",
    "    X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']], sparse=True).values.astype(np.float32))\n",
    "    merge.drop(['item_condition_id', 'shipping'], axis=1, inplace=True)\n",
    "    X_hand_feature = merge[hand_feature].values.astype(np.float32)\n",
    "    merge.drop(hand_feature, axis=1, inplace=True)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # coo_matrix\n",
    "    X_train = hstack((X_dummies[:TRAIN_SIZE], \n",
    "                    X_brand[:TRAIN_SIZE], \n",
    "                    X_brand2[:TRAIN_SIZE], \n",
    "                    X_category1[:TRAIN_SIZE],\n",
    "                    X_category2[:TRAIN_SIZE], \n",
    "                    X_category3[:TRAIN_SIZE],\n",
    "                    X_category4[:TRAIN_SIZE], \n",
    "                    X_hand_feature[:TRAIN_SIZE],\n",
    "                    X_name[:TRAIN_SIZE], \n",
    "                    X_description_train), dtype=np.float32)\n",
    "    print(X_description_train.shape)\n",
    "    X_description_train=None\n",
    "    gc.collect()\n",
    "    print('-' * 50)\n",
    "    X_test1 = hstack((X_dummies[TRAIN_SIZE:TRAIN_SIZE+valid_len1], \n",
    "                    X_brand[TRAIN_SIZE:TRAIN_SIZE+valid_len1],\n",
    "                    X_brand2[TRAIN_SIZE:TRAIN_SIZE+valid_len1],\n",
    "                    X_category1[TRAIN_SIZE:TRAIN_SIZE+valid_len1], \n",
    "                    X_category2[TRAIN_SIZE:TRAIN_SIZE+valid_len1],\n",
    "                    X_category3[TRAIN_SIZE:TRAIN_SIZE+valid_len1], \n",
    "                    X_category4[TRAIN_SIZE:TRAIN_SIZE+valid_len1], \n",
    "                    X_hand_feature[TRAIN_SIZE:TRAIN_SIZE+valid_len1],\n",
    "                    X_name[TRAIN_SIZE:TRAIN_SIZE+valid_len1], \n",
    "                    X_description_test1), dtype=np.float32)\n",
    "    X_description_test1=None\n",
    "    gc.collect()\n",
    "    print('-' * 50)\n",
    "    X_test2 = hstack((X_dummies[TRAIN_SIZE+valid_len1:TRAIN_SIZE+valid_len2], \n",
    "                    X_brand[TRAIN_SIZE+valid_len1:TRAIN_SIZE+valid_len2],\n",
    "                    X_brand2[TRAIN_SIZE+valid_len1:TRAIN_SIZE+valid_len2],\n",
    "                    X_category1[TRAIN_SIZE+valid_len1:TRAIN_SIZE+valid_len2], \n",
    "                    X_category2[TRAIN_SIZE+valid_len1:TRAIN_SIZE+valid_len2],\n",
    "                    X_category3[TRAIN_SIZE+valid_len1:TRAIN_SIZE+valid_len2], \n",
    "                    X_category4[TRAIN_SIZE+valid_len1:TRAIN_SIZE+valid_len2], \n",
    "                    X_hand_feature[TRAIN_SIZE+valid_len1:TRAIN_SIZE+valid_len2],\n",
    "                    X_name[TRAIN_SIZE+valid_len1:TRAIN_SIZE+valid_len2], \n",
    "                    X_description_test2), dtype=np.float32)\n",
    "    X_description_test2=None\n",
    "    gc.collect()\n",
    "    print('-' * 50)\n",
    "    X_test3 = hstack((X_dummies[TRAIN_SIZE+valid_len2:], \n",
    "                    X_brand[TRAIN_SIZE+valid_len2:],\n",
    "                    X_brand2[TRAIN_SIZE+valid_len2:],\n",
    "                    X_category1[TRAIN_SIZE+valid_len2:], \n",
    "                    X_category2[TRAIN_SIZE+valid_len2:],\n",
    "                    X_category3[TRAIN_SIZE+valid_len2:], \n",
    "                    X_category4[TRAIN_SIZE+valid_len2:], \n",
    "                    X_hand_feature[TRAIN_SIZE+valid_len2:],\n",
    "                    X_name[TRAIN_SIZE+valid_len2:], \n",
    "                    X_description_test3), dtype=np.float32)\n",
    "    X_description_test3=None\n",
    "    gc.collect()\n",
    "\n",
    "    print(X_dummies.shape, X_brand.shape, X_brand2.shape, X_category1.shape, X_category2.shape, X_category3.shape, X_category4.shape,\n",
    "          X_hand_feature.shape, X_name.shape, X_train.shape, X_test1.shape, X_test2.shape, X_test3.shape)\n",
    "    X_dummies, X_brand, X_brand2, X_category1, X_category2, X_category3, X_category4, X_hand_feature, X_name = None,None, None,None, None, None, None, None, None\n",
    "    gc.collect()\n",
    "\n",
    "    # csr_matrix\n",
    "    X_train = X_train.tocsr()\n",
    "    print('[{}] X_train completed.'.format(time.time() - start_time))\n",
    "    X_test1 = X_test1.tocsr()\n",
    "    print('[{}] X_test1 completed.'.format(time.time() - start_time))\n",
    "    X_test2 = X_test2.tocsr()\n",
    "    print('[{}] X_test2 completed.'.format(time.time() - start_time))\n",
    "    X_test3 = X_test3.tocsr()\n",
    "    print('[{}] X_test3 completed.'.format(time.time() - start_time))\n",
    "    return X_train, X_test1, X_test2, X_test3\n",
    "\n",
    "\n",
    "class vanila_conv1d_Regressor:\n",
    "    def __init__(self, param_dict, Item_size):\n",
    "        self.seed = 2018\n",
    "        self.batch_size = int(param_dict['batch_size'] * 1.25)\n",
    "        self.lr = param_dict['lr']\n",
    "\n",
    "        name_seq_len = param_dict['name_Len']\n",
    "        desc_seq_len = param_dict['description_Len']\n",
    "        denselayer_units = param_dict['denselayer_units']\n",
    "        embed_name = param_dict['embed_name']\n",
    "        embed_desc = param_dict['embed_desc']\n",
    "        embed_brand = param_dict['embed_brand']\n",
    "        embed_cat_2 = param_dict['embed_cat_2']\n",
    "        embed_cat_3 = param_dict['embed_cat_3']\n",
    "        name_filter = param_dict['name_filter']\n",
    "        desc_filter = param_dict['desc_filter']\n",
    "        name_filter_size = param_dict['name_filter_size']\n",
    "        desc_filter_size = param_dict['desc_filter_size']\n",
    "        dense_drop = param_dict['dense_drop']\n",
    "\n",
    "        name_voc_size = Item_size['name']\n",
    "        desc_voc_size = Item_size['item_description']\n",
    "        brand_voc_size = Item_size['brand_name']\n",
    "        cat1_voc_size = Item_size['category_1']\n",
    "        cat2_voc_size = Item_size['category_2']\n",
    "        cat3_voc_size = Item_size['category_name']\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        graph = tf.Graph()\n",
    "        graph.seed = self.seed\n",
    "        with graph.as_default():\n",
    "            self.place_name = tf.placeholder(tf.int32, shape=(None, name_seq_len))\n",
    "            self.place_desc = tf.placeholder(tf.int32, shape=(None, desc_seq_len))\n",
    "            self.place_brand = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "            self.place_cat1 = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "            self.place_cat2 = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "            self.place_cat3 = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "            self.place_ship = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "            self.place_cond = tf.placeholder(tf.uint8, shape=(None, 1))\n",
    "            self.place_hand = tf.placeholder(tf.float32, shape=(None, len(Item_size['hand_feature'])))\n",
    "\n",
    "            self.place_y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "\n",
    "            self.place_lr = tf.placeholder(tf.float32, shape=(), )\n",
    "\n",
    "            self.is_train = tf.placeholder(tf.bool, shape=(), )\n",
    "\n",
    "            name = embed(self.place_name, name_voc_size, embed_name)\n",
    "            desc = embed(self.place_desc, desc_voc_size, embed_desc)\n",
    "            brand = embed(self.place_brand, brand_voc_size, embed_brand)\n",
    "            cat_2 = embed(self.place_cat2, cat2_voc_size, embed_cat_2)\n",
    "            cat_3 = embed(self.place_cat3, cat3_voc_size, embed_cat_3)\n",
    "\n",
    "            name = conv1d(name, num_filters=name_filter, filter_size=name_filter_size)\n",
    "            name = tf.layers.average_pooling1d(name, pool_size=int(name.shape[1]), strides=1, padding='valid')\n",
    "            name = tf.contrib.layers.flatten(name)\n",
    "\n",
    "            desc = conv1d(desc, num_filters=desc_filter, filter_size=desc_filter_size)\n",
    "            desc = tf.layers.average_pooling1d(desc, pool_size=int(desc.shape[1]), strides=1, padding='valid')\n",
    "            desc = tf.contrib.layers.flatten(desc)\n",
    "\n",
    "            brand = tf.contrib.layers.flatten(brand)\n",
    "\n",
    "            cat_1 = tf.one_hot(self.place_cat1, cat1_voc_size)\n",
    "            cat_1 = tf.contrib.layers.flatten(cat_1)\n",
    "\n",
    "            cat_2 = tf.contrib.layers.flatten(cat_2)\n",
    "            cat_3 = tf.contrib.layers.flatten(cat_3)\n",
    "\n",
    "            hand_feat = self.place_hand\n",
    "            ship = self.place_ship\n",
    "\n",
    "            cond = tf.one_hot(self.place_cond, 5)\n",
    "            cond = tf.contrib.layers.flatten(cond)\n",
    "\n",
    "            out = tf.concat([name, desc, brand, cat_1, cat_2, cat_3, ship, cond, hand_feat], axis=1)\n",
    "            out = dense(out, denselayer_units, activation=tf.nn.relu)\n",
    "            out = tf.layers.dropout(out, rate=dense_drop, training=self.is_train)\n",
    "            self.out = dense(out, 1)\n",
    "\n",
    "            loss = tf.losses.mean_squared_error(self.place_y, self.out)\n",
    "            opt = tf.train.AdamOptimizer(learning_rate=self.place_lr)\n",
    "            self.train_step = opt.minimize(loss)\n",
    "\n",
    "            init = tf.global_variables_initializer()\n",
    "\n",
    "        config = tf.ConfigProto(intra_op_parallelism_threads=THREAD,\n",
    "                                inter_op_parallelism_threads=THREAD,\n",
    "                                allow_soft_placement=True, )\n",
    "        self.session = tf.Session(config=config, graph=graph)\n",
    "        self.init = init\n",
    "\n",
    "    def fit(self, X_train, y_train,X_valid,VALID_SIZE):\n",
    "        self.session.run(self.init)\n",
    "\n",
    "        y_pred = np.zeros(VALID_SIZE)\n",
    "        test_idx = np.arange(VALID_SIZE)\n",
    "        test_batches = prepare_batches(test_idx, self.batch_size)\n",
    "\n",
    "        total_batches=math.ceil(TRAIN_SIZE/self.batch_size)\n",
    "        pred_epoch=total_batches-1-400\n",
    "        weight=0.8\n",
    "\n",
    "        for epoch in range(EPOCH):\n",
    "            np.random.seed(epoch)\n",
    "\n",
    "            train_idx_shuffle = np.arange(TRAIN_SIZE)\n",
    "            np.random.shuffle(train_idx_shuffle)\n",
    "            batches = prepare_batches(train_idx_shuffle, self.batch_size)\n",
    "\n",
    "            for rnd, idx in enumerate(batches):\n",
    "                feed_dict = {\n",
    "                    self.place_name: X_train['X_seq_name'][idx],\n",
    "                    self.place_desc: X_train['X_seq_item_description'][idx],\n",
    "                    self.place_brand: X_train['X_brand_name'][idx],\n",
    "                    self.place_cat1: X_train['X_category_1'][idx],\n",
    "                    self.place_cat2: X_train['X_category_2'][idx],\n",
    "                    self.place_cat3: X_train['X_category_name'][idx],\n",
    "                    self.place_cond: X_train['X_item_condition_id'][idx],\n",
    "                    self.place_ship: X_train['X_shipping'][idx],\n",
    "                    self.place_hand: X_train['X_hand_feature'][idx],\n",
    "                    self.place_y: y_train[idx],\n",
    "                    self.place_lr: self.lr,\n",
    "                    self.is_train: True,\n",
    "                }\n",
    "                self.session.run(self.train_step, feed_dict=feed_dict)\n",
    "\n",
    "                if epoch==1 and rnd == pred_epoch:\n",
    "                    print(pred_epoch)\n",
    "                    for idx in test_batches:\n",
    "                        feed_dict = {\n",
    "                            self.place_name: X_valid['X_seq_name'][idx],\n",
    "                            self.place_desc: X_valid['X_seq_item_description'][idx],\n",
    "                            self.place_brand: X_valid['X_brand_name'][idx],\n",
    "                            self.place_cat1: X_valid['X_category_1'][idx],\n",
    "                            self.place_cat2: X_valid['X_category_2'][idx],\n",
    "                            self.place_cat3: X_valid['X_category_name'][idx],\n",
    "                            self.place_cond: X_valid['X_item_condition_id'][idx],\n",
    "                            self.place_ship: X_valid['X_shipping'][idx],\n",
    "                            self.place_hand: X_valid['X_hand_feature'][idx],\n",
    "                            self.is_train: False,\n",
    "                        }\n",
    "                        batch_pred = self.session.run(self.out, feed_dict=feed_dict)\n",
    "                        y_pred[idx] += batch_pred[:, 0]*weight\n",
    "                    pred_epoch+=200\n",
    "                    weight+=0.2\n",
    "\n",
    "        y_pred/=3\n",
    "        self.session.close()\n",
    "        return y_pred\n",
    "\n",
    "class Middle_Predict(callbacks.Callback):\n",
    "    def __init__(self,X_valid,valid_len,batch_size,train_len):\n",
    "        super().__init__()\n",
    "        self.X_valid=X_valid\n",
    "        self.batch_size=batch_size\n",
    "        self.pred=np.zeros(valid_len)\n",
    "        self.epoch=0\n",
    "        self.weight=0.8\n",
    "\n",
    "        total_batches=math.ceil(train_len/batch_size)\n",
    "        self.pred_epoch =total_batches - 1 - 600\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if self.epoch==1 and batch == self.pred_epoch:\n",
    "            print(batch)\n",
    "            self.pred=self.pred+self.model.predict(self.X_valid, batch_size=self.batch_size).reshape(-1)*self.weight\n",
    "            self.pred_epoch+=300\n",
    "            self.weight+=0.2\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch+=1\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        self.pred/=3\n",
    "\n",
    "class vanila_GRU_Regressor:\n",
    "    def __init__(self, param_dict, Item_size):\n",
    "        config = tf.ConfigProto(intra_op_parallelism_threads=THREAD,\n",
    "                                inter_op_parallelism_threads=THREAD,\n",
    "                                allow_soft_placement=True, )\n",
    "        session = tf.Session(config=config)\n",
    "        backend.set_session(session)\n",
    "\n",
    "        self.batch_size = int(param_dict['batch_size'] * 1.25)\n",
    "        hand_feature_cols = len(Item_size['hand_feature'])\n",
    "\n",
    "        name_seq_len = param_dict['name_Len']\n",
    "        desc_seq_len = param_dict['description_Len']\n",
    "        denselayer_units = param_dict['denselayer_units']\n",
    "        embed_name = param_dict['embed_name']\n",
    "        embed_desc = param_dict['embed_desc']\n",
    "        embed_brand = param_dict['embed_brand']\n",
    "        embed_cat_2 = param_dict['embed_cat_2']\n",
    "        embed_cat_3 = param_dict['embed_cat_3']\n",
    "        rnn_dim_name = param_dict['rnn_dim_name']\n",
    "        rnn_dim_desc = param_dict['rnn_dim_desc']\n",
    "        dense_drop = param_dict['dense_drop']\n",
    "\n",
    "        name_voc_size = Item_size['name']\n",
    "        desc_voc_size = Item_size['item_description']\n",
    "        brand_voc_size = Item_size['brand_name']\n",
    "        cat1_voc_size = Item_size['category_1']\n",
    "        cat2_voc_size = Item_size['category_2']\n",
    "        cat3_voc_size = Item_size['category_name']\n",
    "\n",
    "        # Inputs\n",
    "        X_seq_name = Input(shape=[name_seq_len], name=\"X_seq_name\", dtype='int32')\n",
    "        X_seq_item_description = Input(shape=[desc_seq_len], name=\"X_seq_item_description\", dtype='int32')\n",
    "        X_brand_name = Input(shape=[1], name=\"X_brand_name\", dtype='int32')\n",
    "        X_category_1 = Input(shape=[1], name=\"X_category_1\", dtype='int32')\n",
    "        X_category_2 = Input(shape=[1], name=\"X_category_2\", dtype='int32')\n",
    "        X_category_name = Input(shape=[1], name=\"X_category_name\", dtype='int32')\n",
    "        X_item_condition_id = Input(shape=[1], name=\"X_item_condition_id\", dtype='uint8')\n",
    "        X_shipping = Input(shape=[1], name=\"X_shipping\", dtype='float32')\n",
    "        X_hand_feature = Input(shape=[hand_feature_cols], name=\"X_hand_feature\", dtype='float32')\n",
    "\n",
    "        # Embeddings layers\n",
    "        name = Embedding(name_voc_size, embed_name)(X_seq_name)\n",
    "        item_desc = Embedding(desc_voc_size, embed_desc)(X_seq_item_description)\n",
    "        brand = Embedding(brand_voc_size, embed_brand)(X_brand_name)\n",
    "        cat_2 = Embedding(cat2_voc_size, embed_cat_2)(X_category_2)\n",
    "        cat_3 = Embedding(cat3_voc_size, embed_cat_3)(X_category_name)\n",
    "\n",
    "        # RNN layers\n",
    "        name = GRU(rnn_dim_name)(name)\n",
    "        item_desc = GRU(rnn_dim_desc)(item_desc)\n",
    "\n",
    "        # OneHot layers\n",
    "        cond = Lambda(one_hot, arguments={'num_classes': 5}, output_shape=(1, 5))(X_item_condition_id)\n",
    "        cat_1 = Lambda(one_hot, arguments={'num_classes': cat1_voc_size}, output_shape=(1, cat1_voc_size))(X_category_1)\n",
    "\n",
    "        # main layer\n",
    "        main_l = concatenate([\n",
    "            name,\n",
    "            item_desc,\n",
    "            Flatten()(cat_1),\n",
    "            Flatten()(cat_2),\n",
    "            Flatten()(cat_3),\n",
    "            Flatten()(brand),\n",
    "            Flatten()(cond),\n",
    "            X_shipping,\n",
    "            X_hand_feature,\n",
    "        ])\n",
    "        main_l = Dropout(dense_drop)(Dense(denselayer_units, activation='relu')(main_l))\n",
    "        output = Dense(1, activation=\"linear\")(main_l)\n",
    "\n",
    "        # model\n",
    "        model = Model(\n",
    "            [X_seq_name, X_seq_item_description, X_brand_name, X_category_1, X_category_2, X_category_name,\n",
    "             X_item_condition_id, X_shipping, X_hand_feature],\n",
    "            output)\n",
    "        optimizer = optimizers.Adam(lr=param_dict['lr'])\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test, valid_len):\n",
    "        t0 = time.time()\n",
    "        ensemble_predict = Middle_Predict(X_test, valid_len, self.batch_size, TRAIN_SIZE)\n",
    "        self.model.fit(X_train, y_train, epochs=EPOCH, batch_size=self.batch_size, verbose=2, callbacks=[ensemble_predict])\n",
    "        took = time.time() - t0\n",
    "        print('took %.3fs' % (took))\n",
    "        pred=ensemble_predict.pred\n",
    "        ensemble_predict=None\n",
    "        backend.clear_session()\n",
    "        return pred\n",
    "\n",
    "\n",
    "class vanila_FM_FTRL_Regressor:\n",
    "    def __init__(self, param_dict, D):\n",
    "        alpha = param_dict['alpha']\n",
    "        beta = param_dict['beta']\n",
    "        L1 = param_dict['L1']\n",
    "        L2 = param_dict['L2']\n",
    "        alpha_fm = param_dict['alpha_fm']\n",
    "        init_fm = param_dict['init_fm']\n",
    "        D_fm = param_dict['D_fm']\n",
    "        e_noise = param_dict['e_noise']\n",
    "        iters = param_dict['iters']\n",
    "\n",
    "        self.model = FM_FTRL(alpha=alpha,\n",
    "                             beta=beta,\n",
    "                             L1=L1,\n",
    "                             L2=L2,\n",
    "                             D=D,\n",
    "                             alpha_fm=alpha_fm,\n",
    "                             L2_fm=0.0,\n",
    "                             init_fm=init_fm,\n",
    "                             D_fm=D_fm,\n",
    "                             e_noise=e_noise,\n",
    "                             iters=iters,\n",
    "                             inv_link=\"identity\",\n",
    "                             threads=THREAD)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    merge, submission, y_train = get_extract_feature()\n",
    "    print('[{}] data preparation done.'.format(time.time() - start_time))\n",
    "\n",
    "    if DEVELOP:\n",
    "        VALID_SIZE = 10000\n",
    "    else:\n",
    "        VALID_SIZE = submission.shape[0]\n",
    "\n",
    "    merge, Item_size, hand_feature = Preprocess_features(merge, start_time)\n",
    "    Item_size['hand_feature'] = hand_feature\n",
    "\n",
    "    X_train, X_test = Split_Train_Test_NN(merge, hand_feature)\n",
    "    print('[{}] Split_Train_Test completed'.format(time.time() - start_time))\n",
    "\n",
    "    print('[{}] training conv1d.'.format(time.time() - start_time))\n",
    "    model = vanila_conv1d_Regressor(param_space_best_vanila_con1d, Item_size)\n",
    "    predsConv1d =model.fit(X_train, y_train,X_test, VALID_SIZE).astype(np.float32)\n",
    "    print('[{}] Train conv1d completed'.format(time.time() - start_time))\n",
    "\n",
    "    print('[{}] training GRU.'.format(time.time() - start_time))\n",
    "    model = vanila_GRU_Regressor(param_space_best_vanila_GRU, Item_size)\n",
    "    predsGRU=model.fit(X_train, y_train, X_test,VALID_SIZE).astype(np.float32)\n",
    "    print('[{}] Train GRU completed'.format(time.time() - start_time))\n",
    "    X_train, X_test, model, Item_size = None, None, None, None\n",
    "    merge.drop(['Lb_category_1', 'Lb_category_2', 'Lb_category_name',\n",
    "                'Lb_brand_name', 'Seq_item_description', 'Seq_name'],\n",
    "               axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "\n",
    "    X_train, X_test1, X_test2, X_test3 = Split_Train_Test_FTRL(merge, hand_feature, start_time)\n",
    "    print('[{}] Split_Train_Test completed'.format(time.time() - start_time))\n",
    "    merge, hand_feature = None, None\n",
    "    gc.collect()\n",
    "\n",
    "    print('[{}] training FM_FTRL.'.format(time.time() - start_time))\n",
    "    model = vanila_FM_FTRL_Regressor(param_space_best_FM_FTRL, D=X_train.shape[1])\n",
    "    X_train = X_train.astype(np.float64)\n",
    "    y_train = y_train.reshape(-1).astype(np.float64)\n",
    "    model.fit(X_train, y_train)\n",
    "    print('[{}] Train FM_FTRL completed'.format(time.time() - start_time))\n",
    "    X_train, y_train = None, None\n",
    "    gc.collect()\n",
    "    X_test1 = X_test1.astype(np.float64)\n",
    "    predsFM_FTRL1 = model.predict(X_test1)\n",
    "    X_test1= None\n",
    "    gc.collect()\n",
    "    X_test2 = X_test2.astype(np.float64)\n",
    "    predsFM_FTRL2 = model.predict(X_test2)\n",
    "    X_test2= None\n",
    "    gc.collect()\n",
    "    X_test3 = X_test3.astype(np.float64)\n",
    "    predsFM_FTRL3 = model.predict(X_test3)\n",
    "    X_test3= None\n",
    "    gc.collect()\n",
    "    predsFM_FTRL=np.concatenate((predsFM_FTRL1,predsFM_FTRL2,predsFM_FTRL3))\n",
    "    predsFM_FTRL1, predsFM_FTRL2, predsFM_FTRL3=None,None,None\n",
    "    print('[{}] Predict FM_FTRL completed'.format(time.time() - start_time))\n",
    "\n",
    "    # preds = predsGRU\n",
    "    preds = param_space_best_ensemble['FM_FTRL_weight'] * predsFM_FTRL + \\\n",
    "            param_space_best_ensemble['conv1d_weight'] * predsConv1d + \\\n",
    "            param_space_best_ensemble['GRU_weight'] * predsGRU\n",
    "\n",
    "    submission['price'] = np.expm1(preds * PRICE_STD + PRICE_MEAN)\n",
    "    print(submission['price'].mean())\n",
    "    # submission=submission[: 693359]\n",
    "    submission['price'] = submission['price'].apply(lambda x:max(x,3.0))\n",
    "    submission['price'] = submission['price'].apply(lambda x:min(x,2000.0))\n",
    "    submission.to_csv(\"submission_gru_lstm.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-10T12:37:34.085474Z",
     "start_time": "2018-02-10T12:37:34.064460Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "merge['item_condition_id']\n",
    "# merge['shipping'].fillna(value=0, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
